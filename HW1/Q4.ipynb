{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_HW1_Q4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhGDlWRoAS5X"
      },
      "source": [
        "**Arman Heydari 97521252**\n",
        "\n",
        "first import requirements and dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8_vrtGnnVAk"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDdIkCp8B08H",
        "outputId": "69cfd82f-8969-49f1-818f-fd09f8f3e636"
      },
      "source": [
        "dataset = iris['data']\n",
        "labels = iris['target']\n",
        "print(\"dataset's shape: \", np.shape(dataset))\n",
        "print(\"label's shape: \", np.shape(labels))\n",
        "print(\"feature_names: \", iris[\"feature_names\"])\n",
        "print(\"target_names: \", iris[\"target_names\"])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset's shape:  (150, 4)\n",
            "label's shape:  (150,)\n",
            "feature_names:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "target_names:  ['setosa' 'versicolor' 'virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FAYjMJOaA7R"
      },
      "source": [
        "we need variance and mean for each of features, but first we slice the array into different parts so we have different classes and calculate mean and variance easy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-uZOyiehEwL"
      },
      "source": [
        "setosa = {}\n",
        "versicolor = {}\n",
        "virginica = {}"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6ceRr3TaAJ8"
      },
      "source": [
        "setosa['mean'] = np.average(dataset[0:50], axis=0)\n",
        "setosa['variance'] = np.var(dataset[0:50], axis=0)\n",
        "\n",
        "versicolor['mean'] = np.average(dataset[50:100], axis=0)\n",
        "versicolor['variance'] = np.var(dataset[50:100], axis=0)\n",
        "\n",
        "virginica['mean'] = np.average(dataset[100:150], axis=0)\n",
        "virginica['variance'] = np.var(dataset[100:150], axis=0)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7iLRCUMh9lT"
      },
      "source": [
        "now we need a method for our naive bayes algorithm, and we use gaussian version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxf0P0RDD9bb"
      },
      "source": [
        "def gaussian_naive_bayes(value, mean, var):\n",
        "  return 1 / math.sqrt(2*math.pi*var) * math.exp(((mean-value)*(value-mean)) / (2*var))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9nUCY_urlTo"
      },
      "source": [
        "the main part of naive bayes algorithm is to find the probability of each data belongs to each class(setosa/versicolor/virginica) respectly to the feature's variance and mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnjkwS3ekIQT",
        "outputId": "f1de4cf1-00fe-42fd-e599-b3d39fa33bd7"
      },
      "source": [
        "results = np.zeros(150)\n",
        "for i in range (0, 150):    #for each object in the dataset\n",
        "  p = [1, 1, 1]\n",
        "  \n",
        "  for j in range (0, 4):    #for each of the features we must calculate the probability\n",
        "    p[0] *= gaussian_naive_bayes(dataset[i][j], setosa['mean'][j], setosa['variance'][j])\n",
        "  p[0] *= float(1/3)\n",
        "  \n",
        "  for j in range (0, 4):\n",
        "    p[1] *= gaussian_naive_bayes(dataset[i][j], versicolor['mean'][j], versicolor['variance'][j])\n",
        "  p[1] *= float(1/3)\n",
        "\n",
        "  for j in range (0, 4):\n",
        "    p[2] *= gaussian_naive_bayes(dataset[i][j], virginica['mean'][j], virginica['variance'][j])\n",
        "  p[2] *= float(1/3)\n",
        "\n",
        "  results[i] = p.index(max(p))       #according to naive bayes algorithm, we choose the class which has the maximum probability\n",
        "\n",
        "results\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 2., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n",
              "       2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
              "       1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2.,\n",
              "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WLwkghkrVkQ"
      },
      "source": [
        "and we can undrestand our accuracy by finding the similarity percent between naive bayes results and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7eI8sHtnq_q",
        "outputId": "6ad73950-86d0-4e00-a203-cbfe3967d0c5"
      },
      "source": [
        "counter = 0\n",
        "false_indexes = []\n",
        "for i in range (0, 150):\n",
        "  if labels[i] == results[i]:\n",
        "    counter+=1\n",
        "  else:\n",
        "    false_indexes.append(i)\n",
        "print(\"test accuracy = \", counter/150)\n",
        "print(\"false indexs: \", false_indexes)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy =  0.96\n",
            "false indexs:  [52, 70, 77, 106, 119, 133]\n"
          ]
        }
      ]
    }
  ]
}