{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_HW2_Q2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLNX3K_1i7cL"
      },
      "source": [
        "import requirements and initialize our dataset and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIqWMwTydp-6"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([[2.3, 1.4, 2.6, 3.1, 1.8, 2.8, 5.4, 6.3, 5.8, 6.7, 4.9, 45.2]]).T\n",
        "y = np.array([[0,0,0,0,0,0,1,1,1,1,1,1]]).T"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNUCj8uBjE45"
      },
      "source": [
        "for logistic regression, all we need is the best weights and best bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7PLnVG_hCVF",
        "outputId": "88d7ee27-2a21-455e-c9ec-fd8b8d577a32"
      },
      "source": [
        "clf_logistic = LogisticRegression(solver='liblinear', random_state=0).fit(x, y)\n",
        "print(\"w =\", clf_logistic.coef_[0])\n",
        "print(\"b =\", clf_logistic.intercept_[0])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [0.44616563]\n",
            "b = -1.368972353956046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh-0K19djOIh"
      },
      "source": [
        "and for linear regression, we assume y'=bx+a and then calculate a and b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAwNZlxuhfcw",
        "outputId": "137afd63-61af-429d-87f1-411db2008faf"
      },
      "source": [
        "clf_linear = LinearRegression().fit(x, y)\n",
        "print(\"b =\", clf_linear.coef_[0,0])\n",
        "print(\"a =\", clf_linear.intercept_[0])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b = 0.01885094674164668\n",
            "a = 0.36128845022604983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Axw4iOqqKY5"
      },
      "source": [
        "to compare the two models, we can evaluate their classification results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNaFRz2fqSc_",
        "outputId": "9d3446f9-ba2e-4c7e-9180-ec12874da84d"
      },
      "source": [
        "print(\"linear score:\", clf_linear.score(x, y))\n",
        "print(\"logistic score:\", clf_logistic.score(x, y))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear score: 0.18945201475354911\n",
            "logistic score: 0.9166666666666666\n"
          ]
        }
      ]
    }
  ]
}